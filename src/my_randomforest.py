# -*- coding: utf-8 -*-
# Copyright (C) 2010, Luis Pedro Coelho <lpc@cmu.edu>
# vim: set ts=4 sts=4 sw=4 expandtab smartindent:
# License: MIT. See COPYING.MIT file in the milk distribution

'''
Random Forest
-------------

Main elements
-------------

rf_learner : A learner object
'''

from __future__ import division
import numpy as np
#from milk.supervised.tree import tree_learner
from my_tree import tree_learner
from milk.supervised.normalise import normaliselabels

class rf_model(object):
    '''
    Random Forest Model

    Created by rf_learner.train(features, labels)
    '''
    def __init__(self, forest, names):
        self.forest = forest
        self.names = names

    def apply(self, features, return_label=False):
        ''' Apply Random Forest Model to predict label for features

        Parameters
        ----------
        features : array
        return_labels : bool, optional
            True to return a 0/1 label for the sample,
            generated by a majority vote of the trees in the forest

            False to return the number of trees in favor of label 1
        '''
        votes = [t.apply(features) for t in self.forest]
        from my_tree import mode_w_cnt

        #i, cnt = mode_w_cnt(votes, len(self.names))
        i = 1
        cnt = np.sum(votes)

        if return_label:
            return self.names[i]
        else:
            return self.names[i], cnt
        

class rf_learner(object):
    '''
    Random Forest Learner

    Attributes
    ----------
    rf : int, optional
        the number of trees to be grown in the forest (called jbt by Breiman)
    mtry : int, optional
        the number of variables to split on at each node. Default is
        the square root of the number of features.  Breiman warns:
        "ATTENTION! DO NOT USE THE DEFAULT VALUES OF MTRY0 IF YOU WANT
        TO OPTIMIZE THE PERFORMANCE OF RANDOM FORESTS. TRY DIFFERENT
        VALUES-GROW 20-30 TREES, AND SELECT THE VALUE OF MTRY THAT
        GIVES THE SMALLEST OOB ERROR RATE."
        (called "subsample" in milk.supervised.tree.tree_learner)
    ndsize : int, optional
        minimum node size to split. Default is one. (called
        "min_split" in milk.supervised.tree.tree_learner
    '''
    # TODO: consider adding the following attributes
    ## criterion : function {labels} x {labels} -> float
    ##     function to measure goodness of split
    ## R : source of randomness, optional
    ##     See `milk.util.get_pyrandom`
    ## weights : sequence, optional
    ##     weight of instance (default: all the same)

    def __init__(self, rf=101, mtry=None, ndsize=None):
        self.rf = rf
        self.mtry = mtry
        self.ndsize = ndsize

    def train(self, features, labels,
              rf=None, mtry=None, ndsize=None, R=np.random,
              normalisedlabels=False, return_label=False, weights=None, **kwargs):
        N,M = features.shape
        
        n = N   # was int(.7*N); why?
        m = mtry or self.mtry or int(round(np.sqrt(M)))
        ndsize = ndsize or self.ndsize or 1
        jbt = rf or self.rf  # why is this called rf in milk.py? / why is it called jbt by Breiman?
        
        tree = tree_learner(min_split=ndsize, return_label=return_label,
                            subsample=m)  # also: criterion
        forest = []
        labels,names = normaliselabels(labels)
        for i in xrange(jbt):
            rows = [R.randint(N) for i in xrange(n)]
            forest.append(tree.train(features[rows], labels[rows],
                                     normalisedlabels=True, weights=weights))
        return rf_model(forest, names)



